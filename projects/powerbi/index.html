<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Data &amp; Power BI | ameya.karkal</title>
<meta name="keywords" content="">
<meta name="description" content="The company that I worked for had an interesting problem to solve. They had recently &ldquo;acquired&rdquo; a consultant who provided custom reporting services to higher education institutions which was highly labor intensive and manual. We were asked to fully automate the workflow incrementally AND onboard existing clients
powerbi azure-storage azure-data-factory azure-web-app powerbi-embeded azure-functions azure-entra
What were the challenges?

The existing process was very custom and manual in nature. The consultant custom transformed data files from the institutions, loaded them into an on prem database to perform transformations and imported the results into a power bi report.
The report was then emailed or shared via powerbi service.
The institution would then go back and forth with the consultant about to tweek the reporting bits. This was NOT scalable

engineering bits

We setup a dotnet based website for presentation would leverage powerbi embedded with app owns the data strategy to render embedded reports
consultant would get the data files from the clients and upload it to azure storage using the web app.
we used azure data factory to orchestrate ETLing the data files into the OLAP data model needed for powerbi
we setup custom dotnet service that would load / refresh powerbi report from the processed output files from azure data factory.

learnings

powerbi is a PITA unless you know what you are doing. there are quite a few levels that you need to get it right for it to work as you expect. It greatly reduced the efforts to put reports in a consumable form, however scaling it has its challenges
we delivered the workflow iteratively where initially consultants were given access to load the files directly into azure, which was later replaced by a webapp.
we continued to deliver the reports by still running the most of the transformations on top of SQL server, thereby avoid releasing large changes with unwanted side effects.
">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/projects/powerbi/">
<meta name="google-site-verification" content="c1Y4Uy69lFhrg0JTlUtN_HbEpAR4g6YjUjDNDztmfMA">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/projects/powerbi/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8TM2N649W7"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-8TM2N649W7');
</script>
<meta property="og:url" content="http://localhost:1313/projects/powerbi/">
  <meta property="og:site_name" content="ameya.karkal">
  <meta property="og:title" content="Data & Power BI">
  <meta property="og:description" content="The company that I worked for had an interesting problem to solve. They had recently “acquired” a consultant who provided custom reporting services to higher education institutions which was highly labor intensive and manual. We were asked to fully automate the workflow incrementally AND onboard existing clients
powerbi azure-storage azure-data-factory azure-web-app powerbi-embeded azure-functions azure-entra
What were the challenges? The existing process was very custom and manual in nature. The consultant custom transformed data files from the institutions, loaded them into an on prem database to perform transformations and imported the results into a power bi report. The report was then emailed or shared via powerbi service. The institution would then go back and forth with the consultant about to tweek the reporting bits. This was NOT scalable engineering bits We setup a dotnet based website for presentation would leverage powerbi embedded with app owns the data strategy to render embedded reports consultant would get the data files from the clients and upload it to azure storage using the web app. we used azure data factory to orchestrate ETLing the data files into the OLAP data model needed for powerbi we setup custom dotnet service that would load / refresh powerbi report from the processed output files from azure data factory. learnings powerbi is a PITA unless you know what you are doing. there are quite a few levels that you need to get it right for it to work as you expect. It greatly reduced the efforts to put reports in a consumable form, however scaling it has its challenges we delivered the workflow iteratively where initially consultants were given access to load the files directly into azure, which was later replaced by a webapp. we continued to deliver the reports by still running the most of the transformations on top of SQL server, thereby avoid releasing large changes with unwanted side effects. ">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects">
    <meta property="article:published_time" content="2021-07-01T00:00:00-05:00">
    <meta property="article:modified_time" content="2021-07-01T00:00:00-05:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Data &amp; Power BI">
<meta name="twitter:description" content="The company that I worked for had an interesting problem to solve. They had recently &ldquo;acquired&rdquo; a consultant who provided custom reporting services to higher education institutions which was highly labor intensive and manual. We were asked to fully automate the workflow incrementally AND onboard existing clients
powerbi azure-storage azure-data-factory azure-web-app powerbi-embeded azure-functions azure-entra
What were the challenges?

The existing process was very custom and manual in nature. The consultant custom transformed data files from the institutions, loaded them into an on prem database to perform transformations and imported the results into a power bi report.
The report was then emailed or shared via powerbi service.
The institution would then go back and forth with the consultant about to tweek the reporting bits. This was NOT scalable

engineering bits

We setup a dotnet based website for presentation would leverage powerbi embedded with app owns the data strategy to render embedded reports
consultant would get the data files from the clients and upload it to azure storage using the web app.
we used azure data factory to orchestrate ETLing the data files into the OLAP data model needed for powerbi
we setup custom dotnet service that would load / refresh powerbi report from the processed output files from azure data factory.

learnings

powerbi is a PITA unless you know what you are doing. there are quite a few levels that you need to get it right for it to work as you expect. It greatly reduced the efforts to put reports in a consumable form, however scaling it has its challenges
we delivered the workflow iteratively where initially consultants were given access to load the files directly into azure, which was later replaced by a webapp.
we continued to deliver the reports by still running the most of the transformations on top of SQL server, thereby avoid releasing large changes with unwanted side effects.
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "http://localhost:1313/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Data \u0026 Power BI",
      "item": "http://localhost:1313/projects/powerbi/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Data \u0026 Power BI",
  "name": "Data \u0026 Power BI",
  "description": "The company that I worked for had an interesting problem to solve. They had recently \u0026ldquo;acquired\u0026rdquo; a consultant who provided custom reporting services to higher education institutions which was highly labor intensive and manual. We were asked to fully automate the workflow incrementally AND onboard existing clients\npowerbi azure-storage azure-data-factory azure-web-app powerbi-embeded azure-functions azure-entra\nWhat were the challenges? The existing process was very custom and manual in nature. The consultant custom transformed data files from the institutions, loaded them into an on prem database to perform transformations and imported the results into a power bi report. The report was then emailed or shared via powerbi service. The institution would then go back and forth with the consultant about to tweek the reporting bits. This was NOT scalable engineering bits We setup a dotnet based website for presentation would leverage powerbi embedded with app owns the data strategy to render embedded reports consultant would get the data files from the clients and upload it to azure storage using the web app. we used azure data factory to orchestrate ETLing the data files into the OLAP data model needed for powerbi we setup custom dotnet service that would load / refresh powerbi report from the processed output files from azure data factory. learnings powerbi is a PITA unless you know what you are doing. there are quite a few levels that you need to get it right for it to work as you expect. It greatly reduced the efforts to put reports in a consumable form, however scaling it has its challenges we delivered the workflow iteratively where initially consultants were given access to load the files directly into azure, which was later replaced by a webapp. we continued to deliver the reports by still running the most of the transformations on top of SQL server, thereby avoid releasing large changes with unwanted side effects. ",
  "keywords": [
    
  ],
  "articleBody": "The company that I worked for had an interesting problem to solve. They had recently “acquired” a consultant who provided custom reporting services to higher education institutions which was highly labor intensive and manual. We were asked to fully automate the workflow incrementally AND onboard existing clients\npowerbi azure-storage azure-data-factory azure-web-app powerbi-embeded azure-functions azure-entra\nWhat were the challenges? The existing process was very custom and manual in nature. The consultant custom transformed data files from the institutions, loaded them into an on prem database to perform transformations and imported the results into a power bi report. The report was then emailed or shared via powerbi service. The institution would then go back and forth with the consultant about to tweek the reporting bits. This was NOT scalable engineering bits We setup a dotnet based website for presentation would leverage powerbi embedded with app owns the data strategy to render embedded reports consultant would get the data files from the clients and upload it to azure storage using the web app. we used azure data factory to orchestrate ETLing the data files into the OLAP data model needed for powerbi we setup custom dotnet service that would load / refresh powerbi report from the processed output files from azure data factory. learnings powerbi is a PITA unless you know what you are doing. there are quite a few levels that you need to get it right for it to work as you expect. It greatly reduced the efforts to put reports in a consumable form, however scaling it has its challenges we delivered the workflow iteratively where initially consultants were given access to load the files directly into azure, which was later replaced by a webapp. we continued to deliver the reports by still running the most of the transformations on top of SQL server, thereby avoid releasing large changes with unwanted side effects. ",
  "wordCount" : "313",
  "inLanguage": "en",
  "datePublished": "2021-07-01T00:00:00-05:00",
  "dateModified": "2021-07-01T00:00:00-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/projects/powerbi/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ameya.karkal",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="ameya.karkal (Alt + H)">ameya.karkal</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/projects/">Projects</a></div>
    <h1 class="post-title entry-hint-parent">
      Data &amp; Power BI
    </h1>
    <div class="post-meta"><span title='2021-07-01 00:00:00 -0500 -0500'>July 1, 2021</span>&nbsp;·&nbsp;2 min

</div>
  </header> 
  <div class="post-content"><p>The company that I worked for had an interesting problem to solve. They had recently &ldquo;acquired&rdquo; a consultant who provided custom reporting services to higher education institutions which was highly labor intensive and manual. <strong>We were asked to fully automate the workflow incrementally AND onboard existing clients</strong></p>
<p><code>powerbi</code> <code>azure-storage</code> <code>azure-data-factory</code> <code>azure-web-app</code> <code>powerbi-embeded</code> <code>azure-functions</code> <code>azure-entra</code></p>
<h2 id="what-were-the-challenges">What were the challenges?<a hidden class="anchor" aria-hidden="true" href="#what-were-the-challenges">#</a></h2>
<ul>
<li>The existing process was very custom and manual in nature. The consultant custom transformed data files from the institutions, loaded them into an on prem database to perform transformations and imported the results into a power bi report.</li>
<li>The report was then emailed or shared via powerbi service.</li>
<li>The institution would then go back and forth with the consultant about to tweek the reporting bits. This was NOT scalable</li>
</ul>
<h2 id="engineering-bits">engineering bits<a hidden class="anchor" aria-hidden="true" href="#engineering-bits">#</a></h2>
<ul>
<li>We setup a dotnet based website for presentation would leverage powerbi embedded with app owns the data strategy to render embedded reports</li>
<li>consultant would get the data files from the clients and upload it to azure storage using the web app.</li>
<li>we used azure data factory to orchestrate ETLing the data files into the OLAP data model needed for powerbi</li>
<li>we setup custom dotnet service that would load / refresh powerbi report from the processed output files from azure data factory.</li>
</ul>
<h2 id="learnings">learnings<a hidden class="anchor" aria-hidden="true" href="#learnings">#</a></h2>
<ul>
<li>powerbi is a PITA unless you know what you are doing. there are quite a few levels that you need to get it right for it to work as you expect. It greatly reduced the efforts to put reports in a consumable form, however scaling it has its challenges</li>
<li>we delivered the workflow iteratively where initially consultants were given access to load the files directly into azure, which was later replaced by a webapp.</li>
<li>we continued to deliver the reports by still running the most of the transformations on top of SQL server, thereby avoid releasing large changes with unwanted side effects.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">ameya.karkal</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
